## 假设函数

* 神经网络本身，即是假设函数能够计算输入相对的输出。

## 代价函数

* $L$表示神经网络的总层数。
* $s_L$表示第L层单元的个数。
* $K$表示输出层单元的个数
* 代价函数相当于第i组数据输入时，产生的误差。

$$
J(\theta)=-\frac{1}{m}[\sum_i^my^{(i)}\log h_\theta(x^{(i)})+(1-y^{(i)})\log (1-h_\theta (x^{(i)}))]+\frac{\lambda}{2m}\sum_1^n\theta_j^2
$$
* $a^{(i)}$表示第i层的单元值。
* $\Theta^{(i)}$第i层的权重
* $z^{(i)}$第i层的加权值
* $\delta^{(i)}$第i层的反向传播误差。

## 最小化代价函数：反向传播算法


* 在这里的上标，代表的不是输入的代数（即第几次迭代），而是神经网络的层数。下标表示的是神经网络某层的单元数。
* 原理：神经网络的值会随着假设函数正向传播。神经网络的误差会随着假设函数反向传播到第二层。利用每一层的单元值和神经网络的误差能够计算每一层的梯度下降向量，通过梯度下降向量，完成参数的更新。
* 神经网络的正向传播过程

![](../img/forward.png)

* 神经网络的反向传播过程

![](../img/backward.png)

* 神经网络反向传播算法实现

![](../img/backwark_过程.png)


## 反向传播算法理解

* $\delta_j^{(i)}$表示第i层的单元j的误差。他相当于单元j的代价函数$J=cost_j$关于加权值$z^{(i)}_j$的偏导数

$$
\delta_j^{(i)}=\frac{\partial}{\partial z}cost_j
$$


![](../img/backward_原理.png)

## 梯度检测

* 使用差分方法，近似某个点的梯度。普通的梯度是通过求导公式得到导数，然后进行梯度下降。可以使用差分近似导数，与梯度进行对比，完成梯度检测。

## 权重随机初始化

* 避免权重相同，出现高度冗余。
* 因为在梯度下降更新过程中，相同的权重，会进行相同程度的更新。
* 打破对称性流程

## 神经网络的组合

### 步骤

> 使用训练集中的每一个样本迭代训练。

1. 选择神经网络的架构，随机初始化权重。输入层单元、隐藏层单元、输出层单元。选择一个隐藏层，或者多个隐藏层具有相同的单元数。
2. 执行前向传播算法，计算假设函数的值。
3. 计算代价函数。
4. 通过反向传播算法，计算每一层的梯度（偏导数）。
5. 使用梯度检测，对比梯度的偏导数。检测梯度算法的正确性。然后关闭梯度下降算法。
6. 使用梯度下降算法或者其他更高级的优化算法+反向传播算法计算出的梯度。来优化权重参数。

## 编程任务

* 完成以上的六个步骤，寻找合适的训练数据集。


