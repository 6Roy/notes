{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.0 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "38740d3277777e2cd7c6c2cc9d8addf5118fdf3f82b1b39231fd12aeac8aee8b"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# 3 Tensor"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([3, 4]) torch.float32 cpu\ntensor([[1., 0., 1., 1.],\n        [1., 0., 1., 1.],\n        [1., 0., 1., 1.],\n        [1., 0., 1., 1.]])\ntensor([[1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.]])\ntensor([[[1., 0., 1., 1.]],\n\n        [[1., 0., 1., 1.]],\n\n        [[1., 0., 1., 1.]],\n\n        [[1., 0., 1., 1.]]])\ntensor([[6., 5., 6., 6.],\n        [6., 5., 6., 6.],\n        [6., 5., 6., 6.],\n        [6., 5., 6., 6.]])\n[[6. 5. 6. 6.]\n [6. 5. 6. 6.]\n [6. 5. 6. 6.]\n [6. 5. 6. 6.]]\n[[9. 8. 9. 9.]\n [9. 8. 9. 9.]\n [9. 8. 9. 9.]\n [9. 8. 9. 9.]]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np \n",
    "\n",
    "# 张量初始化的方法\n",
    "# 直接生成张量\n",
    "data = [[1,2],[3,4]]\n",
    "x_data = torch.tensor(data)\n",
    "\n",
    "# numpy数组转化\n",
    "np_array = np.array(data)\n",
    "x_np=torch.from_numpy(np_array)\n",
    "\n",
    "# 通过已有的张量生成新的张量\n",
    "x_ones = torch.ones_like(x_data)\n",
    "x_rand = torch.rand_like(x_data,dtype = torch.float)\n",
    "\n",
    "# 通过制定数组的维度生成张量\n",
    "shape = (2,3)\n",
    "rand_tensor = torch.rand(size=shape)\n",
    "ones_tensor = torch.ones(shape)\n",
    "zeros_tensor = torch.zeros(shape)\n",
    "\n",
    "# 张量的属性\n",
    "tensor = torch.rand(3,4)\n",
    "print(tensor.shape,tensor.dtype,tensor.device)\n",
    "\n",
    "# 张量运算\n",
    "# 将tensor导入gpu内运行\n",
    "if torch.cuda.is_available():\n",
    "    tensor = tensor.to('cuda')\n",
    "# 张量的索引和切片-与numpy数组完全一致\n",
    "tensor = torch.ones(4,4)\n",
    "tensor[:,1]=0\n",
    "print(tensor)\n",
    "\n",
    "# 张量的拼接_在某个内层已有的维度上延伸\n",
    "t1 = torch.cat([tensor,tensor,tensor],dim=1)\n",
    "print(t1)\n",
    "\n",
    "# 张量的拼接_创建一个新的维度，组装原来的tensor。在某个维度上组装原来的tensor。原来的tensor不会被破坏\n",
    "t2 = torch.stack((tensor,),dim=1)\n",
    "print(t2)\n",
    "\n",
    "# 张量的乘积和矩阵乘法\n",
    "# 逐个元素相乘\n",
    "tensor.mul(tensor)\n",
    "tensor*tensor\n",
    "# 矩阵乘法\n",
    "tensor.matmul(tensor.T)\n",
    "tensor @ tensor.T\n",
    "\n",
    "# 自赋值运算 _自赋值。否则返回运算后的值。\n",
    "tensor.add_(5)\n",
    "print(tensor)\n",
    "\n",
    "# tensor与numpy的相互转换。而且numpy与tensor公用同样的地址的值\n",
    "np_array = tensor.numpy()\n",
    "print(np_array)\n",
    "tensor.add_(3)\n",
    "print(np_array)"
   ]
  },
  {
   "source": [
    "## 4 torch.autograd "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 正向传播和方向传播的用法\n",
    "import torch,torchvision\n",
    "model = torchvision.models.resnet18(pretrained=True)\n",
    "\n",
    "data = torch.rand(1,3,64,64)\n",
    "labels = torch.rand(1,1000)\n",
    "# print(data)\n",
    "prediction = model(data)\n",
    "# print(prediction.type)\n",
    "loss = (prediction-labels).sum()\n",
    "loss.backward()\n",
    "optim = torch.optim.SGD(model.parameters(),lr=1e-2,momentum=0.9)\n",
    "optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "None\nNone\ntensor([36., 81.])\ntensor([-12.,  -8.])\ntensor([True, True])\ntensor([True, True])\n"
     ]
    }
   ],
   "source": [
    "# autograd实现微分\n",
    "# tensor 能够通过autograd自动梯度。进行梯度下降分析。即记住运算过程。\n",
    "# 每个张量都有自己的grad属性。用来存储本张量的梯度下降值。\n",
    "import torch\n",
    "\n",
    "a = torch.tensor([2.,3.],requires_grad=True)\n",
    "b = torch.tensor([6.,4.],requires_grad=True)\n",
    "\n",
    "\n",
    "Q = 3*a**3 -b**2\n",
    "\n",
    "# 构建一个梯度运算的图（树）。\n",
    "print(a.grad)\n",
    "print(b.grad)\n",
    "\n",
    "# Q.sum().backward()\n",
    "external_grad = torch.tensor([1.,1.])\n",
    "Q.backward(gradient=external_grad)\n",
    "\n",
    "print(a.grad)\n",
    "print(b.grad)\n",
    "\n",
    "print(9*a**2 == a.grad)\n",
    "print(-2*b == b.grad)"
   ]
  },
  {
   "source": [
    "> 计算图.自动维护一个计算图。输入节点为叶节点。输出节点为根节点。的树。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 5 神经网络"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Net(\n  (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))\n  (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))\n  (fc1): Linear(in_features=576, out_features=120, bias=True)\n  (fc2): Linear(in_features=120, out_features=84, bias=True)\n  (fc3): Linear(in_features=84, out_features=10, bias=True)\n)\n10\ntorch.Size([6, 1, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net,self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1,6,3)\n",
    "        self.conv2 = nn.Conv2d(6,16,3)\n",
    "        self.fc1 = nn.Linear(16*6*6,120)\n",
    "        self.fc2 = nn.Linear(120,84)\n",
    "        self.fc3 = nn.Linear(84,10)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)),(2,2))\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)),2)\n",
    "        x = x.view(-1,self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x =  self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    def num_flat_features(self,x):\n",
    "        size = x.size()[1:]\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *=s\n",
    "        return num_features\n",
    "\n",
    "net = Net()\n",
    "print(net)\n",
    "params = list(net.parameters())\n",
    "print(len(params))\n",
    "print(params[0].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[ 0.0492, -0.0594, -0.1026, -0.0511,  0.0224, -0.0672,  0.1048,  0.0772,\n         -0.1358, -0.0327]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "input = torch.randn(1,1,32,32)\n",
    "out = net(input)\n",
    "print(out)\n",
    "\n",
    "net.zero_grad()\n",
    "out.backward(torch.randn(1,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[0.9332, 0.9385],\n        [0.5645, 0.9817],\n        [0.0998, 0.0800],\n        [0.3189, 0.7160],\n        [0.4157, 0.3705]])\n"
     ]
    }
   ],
   "source": [
    "# tensor,的第一个维度表示数量。第二个维度开始表示数据的格式。如果第一维的维度>1说明存在一个以上的数据条目\n",
    "a = torch.rand(2,5)\n",
    "# b = np.rand(2,3)\n",
    "print(a.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "None\nTrue\ntensor([[-0.8533, -1.8255,  0.1003, -0.2550,  0.0429, -2.4029, -2.1907, -0.3119,\n         -0.5956,  0.5517]])\ntensor(1.5207, grad_fn=<MseLossBackward>)\n<MseLossBackward object at 0x000001D03A547370>\n<AddmmBackward object at 0x000001D03A44CAC0>\n<AccumulateGrad object at 0x000001D03A23D400>\n"
     ]
    }
   ],
   "source": [
    "# 定义损失\n",
    "print(input.grad)\n",
    "print(input.is_leaf)\n",
    "output = net(input)\n",
    "target = torch.randn(10)\n",
    "target = target.view(1,-1)\n",
    "print(target)\n",
    "criterion = nn.MSELoss()\n",
    "loss = criterion(output,target)\n",
    "# 输出了反向传播过程中的函数\n",
    "print(loss)\n",
    "print(loss.grad_fn)\n",
    "print(loss.grad_fn.next_functions[0][0])\n",
    "print(loss.grad_fn.next_functions[0][0].next_functions[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([0., 0., 0., 0., 0., 0.])\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time, but the saved intermediate results have already been freed. Specify retain_graph=True when calling .backward() or autograd.grad() the first time.",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-100-139d55f804ab>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 245\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    246\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    143\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 145\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time, but the saved intermediate results have already been freed. Specify retain_graph=True when calling .backward() or autograd.grad() the first time."
     ]
    }
   ],
   "source": [
    "# 进行反向传播\n",
    "net.zero_grad()\n",
    "\n",
    "print(net.conv1.bias.grad)\n",
    "loss.backward()\n",
    "\n",
    "print(net.conv1.bias.grad)\n",
    "# 更新网络权重weight = weight -learning_rate*gradient\n",
    "learning_rate = 0.01\n",
    "for f in net.parameters():\n",
    "    f.data.sub_(f.grad.data*learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 常用的优化器\n",
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.SGD(net.parameters(),lr=0.01)\n",
    "\n",
    "optimizer.zero_grad()\n",
    "\n",
    "output = net(input)\n",
    "loss = criterion(output,target)\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "source": [
    "### 卷积的计算公式\n",
    "$$\n",
    "d = (d - kennelsize + 2 * padding) / stride + 1\n",
    "$$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}