# 一元线性回归

## 1 回归分析理解

### 变量关系

* 确定性关系：可用函数来描述。
* 非确定性关系：不能用函数来描述。


### 回归分析

* 回归模型：变量间相关关系无法用完全确定的函数关系描述，但在平均意义下，有一定的函数表示式。通过大量数据，估计函数表示式，称为回归分析。回归分析中根据观测数据建立的反映变量间相关关系的统计模型称为回归模型。

* 回归分析分类
  * 随机变量间的相关关系。
  * 随机变量与普通变量间的相关关系。

> 随机变量与普通变量的相关关系的回归分析：普通变量更像一个参数，能够决定随机分布的参数。建立的函数关系式是参数$E(Y)=\mu$与普通变量之间的关系。


### 相关定义

响应变量和解释变量
* 响应变量+解释变量：在随机变量与普通变量的回复分析中，随机变量是因变量，或响应变量。普通变量为自变量，或解释变量。

回归分析的元
* 一元回归分析：一个响应变量+一个解释变量
* 多元回归分析：一个响应变量+多个解释变量
* 多元多重回复分析：多个响应变量+多个解释变量

回归分析的方法
* 线性回归：线性统计模型
* 多项式回归：
* 神经网络（常数回归？）：
* 支持向量积（不知道诶）：


> 这里很难用同一个分布的随机变量数字特征来表示。之前只讲过一个随机变量的分布，这个分布有期望和方差。这里随机变量随着普通变量变化。并且将随机变量抽象为三个部分：常数部分、参数变化部分、随机性部分。

> 普通变量对随机变量的影响。参数对随机变量的影响。随机变量随着普通变量的变化关系。随机变量随着参数的变化过程。

> 应该假设已经完全掌握之前的知识。另外，知识的巩固应该通过做题。


## 2 一元线性回归的数学描述

$E(y)=\mu(x)$的理解。

随机变量
* E(y):随机变量的数据特征。是随机变量的在某一个性质上投影。
* $\mu(x)$:这个性质投影，能够消除随机变量的随机性，反映一种基本性质。这个性质投影，与某个普通变量存在一定的关系。

* 这与之前的随机变量的参数估计相似。一个随机变量有一个未知参数——数学期望。利用样本数据求出这种关系。

回归分析
* 求函数表达式$E(y)=\mu(x)$的估计是回归分析的基本内容。$\mu(x)$为y对x的理论回归函数，而基于数据对$\mu(x)$估计称为y对x的经验回归函数。

一元回归分析的表示
$$
\begin{cases}
    y = a + b x+\varepsilon\\
    E(\varepsilon)=0,Var(\varepsilon)=\sigma^2
\end{cases}\\
a是截距,b是斜率。
$$

## 3 未知参数的估计及其统计性质
## 3.1 最小二乘估计过程
### 定义：随机变量的表达

* 条件
$$
(x_1,y_1),\cdots,(x_n,y_n)是样本\\
\varepsilon_i相互独立，表示随机性，是随机误差\\
a,b,\sigma^2是未知参数
$$
* 结论
$$
\begin{cases}
    y_i = a + b x_i+\varepsilon_i\\
    E(\varepsilon_i)=0,Var(\varepsilon_i)=\sigma^2
\end{cases},i=1,2,\cdots,n
$$

### 计算：最小二乘估计
偏差平方和最小
$$
Q(a,b)=\sum_{i=1}^n(y_i-a-bx_i)^2
$$
偏导等于零，解得
$$
\hat{a}=\overline{x}-\hat{b}\overline{x}\\
\hat{b}=\frac{\sum x_iy_i-n\overline{x}\overline{y}}{\sum x_i^2-n\overline{x}^2}\\
=\frac{\sum(x_i-\overline{x})(y_i-\overline{y})}{\sum(x_i-\overline{x})^2}\\
=\frac{L_{xy}}{L_{xx}}
$$
> 斜率b的值等于xy的混合阶，除以x的阶。

### 定义：规定符号
$$
L_{xx}=\sum(x_i-\overline{x})^2=\sum x_i^2-n\overline{x}=\sum(x_i-\overline{x})x_i\\

L_{yy}=\sum(y_i-\overline{y})^2=\sum y_i^2-n\overline{y}=\sum(y_i-\overline{y})y_i\\

L_{xy}=\sum(x_i-\overline{x})(y_i-\overline{y})=\sum x_iy_i-n\overline{xy}=\sum(x_i-\overline{x})y_i

$$

## 3.2 最小二乘估计性质
### 定理：最小二乘估计的性质，一致最小方差无偏估计。

* $\hat{a},\hat{b}是a、b的一致最小方差线性无偏估计$
* $Cov(\overline{y},\hat{b})=0$
* $Cov(\hat{a},\hat{b})=-\frac{\overline{x}}{L_{xx}}\sigma^2$
* $Var(\hat{a})=(\frac{1}{n}+\frac{\overline{x}}{L_{xx}})\sigma^2$
* $Var(\hat{b})=\frac{\sigma^2}{L_{xx}}$

> 自由度讨论：协方差等于零，等价于两个变量的不相关，具有相互独立性。

### 注意
1. 选择$x_i$使得$\overline{x}=0$时，$Var(\hat{a})=\frac{\sigma^2}{n}$达到最小值。
2. $x_i$取值越分散越好，$L_{xx}$越大表示越分散。
3. 实验次数n不能太小

### 定义：适应区间估计和假设检验的模型

$$
\begin{cases}
y=a+bx+\varepsilon\\
\varepsilon\sim N(0,\sigma^2)
\end{cases}
$$

### 定理：最小二乘估计也是极大似然估计

* $\hat{a},\hat{b}$也是极大似然估计
* $\hat{a}\sim N(a,(\frac{1}{n}+\frac{\overline{x}^2}{L_{xx}})\sigma^2)$
* $\hat{b}\sim N(b,\frac{\sigma^2}{L_{xx}})$
* $\overline{y}与\hat{b}相互独立$

## 3.3 $\sigma^2$的无偏估计
### 定理：$\sigma^2$的无偏估计
$Q(\hat{a},\hat{b})=\sum(y_i-\hat{y_i}^2)$称为残差平方和。
$$
E(Q)=(n-2)\sigma^2\\
\sigma^2=E(\frac{Q}{n-2})是\sigma^2的一个无偏估计
$$

### 定理：Q的性质

1. $\frac{Q}{\sigma^2}\sim\chi^2(n-2)$
2. $\hat{b}$与$Q$相互独立

## 4 回归的显著性检验与回归系数的置信区间

## 4.1 平方和分解过程 
### 计算：平方和分解，回归系数的由来
1. 平方和分解
$$
L_{yy}=\sum(y-y_i)^2=\sum(y_i-\hat{y_i})^2+\sum(\hat{y}-\overline{y})^2\\
Q=\sum(y_i-\hat{y})^2,U=\sum(\hat{y}-\overline{y})^2\\
L_{yy}=Q+U
$$

2. 相关性$R^2$的定义，表示数据离散长度占总的离散程度。描述了回归平方和U占离差平方和Lyy的比重。称$R^2$为决定系数、拟合优度，用直线来拟合数据的好坏程度。

$$
R^2=\frac{U}{L_{yy}}=\frac{[\sum(x_i-\overline{x})(y_i-\overline{y})]^2}{\sum(x_i-\overline{x})^2\sum(y_i-\overline{y})^2}
$$

3. 将决定系数标准化（单位标准化）。相关系数接近于1，表示相关性越大，相关系数接近于零，相关性越小。
$$
r=\frac{\sum(x_i-\overline{x})(y_i-\overline{y})}{\sqrt{\sum(x_i-\overline{x}})^2\sqrt{\sum(y_i-\overline{y})^2}}=\frac{L_{xy}}{\sqrt{L_{xx}}\sqrt{L_{yy}}}
$$

## 4.2 回归的显著性检验

### 显著性检验假设
> 对于多元线性回归，回归效果的显著性检验与回归系数的显著性检验是不同的。
> * 回归效果的显著性检验检验响应变量对所有的解释变量整体的线性依赖性是否显著。
> * 回归系数的显著性检验主要检验响应变量对某个解释变量或者某几个解释变量的线性依赖是否显著。

$$
H_0:b=0,H_1:b\not =0
$$

### 定理：$\chi^2$检验

在一元线性回归模型下，假设$H_0$成立时，选取$\frac{U}{\sigma^2}$作为检验统计量
$$
\frac{U}{\sigma^2}\sim \chi^2(1)
$$

### 定理：F检验

在一元线性回归模型下，选取$F=\frac{\frac{U}{\sigma^2}/1}{\frac{Q}{\sigma^2}/(n-2)}$作为检验统计量
$$
F=\frac{\frac{U}{\sigma^2}/1}{\frac{Q}{\sigma^2}/(n-2)}=\frac{(n-2)U}{Q}\sim F(1,n-2)
$$

### 定理：r检验
在一元线性回归模型下选取r作为检验统计量。
$$
F=\frac{(n-1)r^2}{1-r^2}\\
P\{|r|\geq [1+\frac{n-2}{F_{1-\alpha}(1,n-2)}]^{-\frac{1}{2}}\}=\alpha
$$

### 定理：t检验法

在一元线性回归模型下，选取$t=\frac{\hat{b}-b}{\hat{\sigma^2}}\sqrt{L_{xx}}$作为检验统计量。
$$
t=\frac{\hat{b}-b}{\hat{\sigma^2}}\sqrt{L_{xx}}\sim t(n-2)
$$

## 4.3 回归系数的置信区间

### 计算：b的置信区间

1. 选择枢轴变量法的统计量$\frac{\hat{b}-b}{\hat{b}}\sigma^2\sqrt{L_{xx}}\sim t(n-2)$
2. 给定置信水平$1-\alpha$有如下的关系式
$$
P\{\frac{|\hat{b}-b|}{\hat{\sigma}}\sqrt{L_{xx}}\leq t_{1-\frac{\alpha}{2}}(n-2)\}=1-\alpha
$$
3. 得到参数水平为$\alpha$的置信区间

$$
[\hat{b}-\frac{\hat{\sigma}}{\sqrt{L_{xx}}}t_{1-\frac{\alpha}{2}}(n-2),\hat{b}+\frac{\hat{\sigma}}{\sqrt{L_{xx}}}t_{1-\frac{\alpha}{2}}(n-2)]
$$

### 计算：a的置信区间

$$
[\hat{a}-\hat{\sigma}t_{1-\frac{\alpha}{2}}(n-2)\sqrt{\frac{1}{n}+\frac{\overline{x}^2}{L_{xx}}},\hat{a}+\hat{\sigma}t_{1-\frac{\alpha}{2}}(n-2)\sqrt{\frac{1}{n}+\frac{\overline{x}^2}{L_{xx}}}]
$$

## 5 预测控制

### 计算：预测-$\hat{y}$的置信区间

> 求$\hat{y}$的置信区间，就是$\varepsilon$的置信区间。

一元线性回归模型成立，$\varepsilon_0\sim N(0,\sigma^2)$。
1. 选择枢轴变量法的统计量$t=\frac{y_0-\hat{y}_0}{\hat{\sigma}\sqrt{1+\frac{1}{n}+\frac{(x_0-\overline{x})^2}{L_{xx}}}}\sim t(n-2)$

2. 给定置信水平$1-\alpha$有如下不等式
$$
P\{t\leq t_{1-\frac{\alpha}{2}(n-2)}\}=1-\alpha
$$

3. 置信区间  
这样会得到一个与样本的x_0相关的置信区间。这个置信区间是置信水平为1-\alpha对应的可能的\varepsilon取值，即对应的\hat(y)的取值。通过这个取值，能够得到一个带状区域。即用来预测的可能的带状区域。
$$
[\hat{y}_0-\delta(x_0),\hat{y}_0+\delta(x_0)]\\
$$

### 计算：控制
给出$y_1^*,y_2^*,\alpha$计算此时应该控制的x的取值范围。

得到条件方程式
$$
P\{y_1^*\leq y\leq y_2^*\}\geq 1-\alpha\\
$$

需要包含y的置信区间
$$
[\hat{y}_0-\delta(x_0),\hat{y}_0+\delta(x_0)]\\
$$

得到不等式

$$
y_1^*+\delta(x)\leq \hat{y}(x) \leq y_2^*-\delta(x)
$$

解得
$$
x_1^* = \frac{1}{\hat{b}}(y_1^*-\hat{a}+\hat{\sigma}z_{1-\frac{\alpha}{2}})\\
x_2^* = \frac{1}{\hat{b}}(y_2^*-\hat{a}+\hat{\sigma}z_{1-\frac{\alpha}{2}})\\
$$

